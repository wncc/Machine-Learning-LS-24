# CLASSIFIERS
In this course, you'll learn how to use classifiers, powerful tools for data science and machine learning. You'll explore various classification algorithms, including logistic regression, decision trees, random forests, and support vector machines. By the end, you'll be well-equipped to handle various data science and machine learning projects involving classification tasks.
## Regression:
>[!Note]
>You can *skip* the regression part if done before  
 
Regression is a key technique in data science and machine learning for modeling relationships between variables. You'll learn logistic regression for binary classification and softmax regression for multi-class classification. By mastering these methods, you'll enhance your ability to analyze and interpret data effectively.

[Logistic Regression](https://youtu.be/zM4VZR0px8E?si=1GkpCaVPTQMuHi6D) (till 16:00 at 2x speed)  
[Softmax Regression](https://youtu.be/J5bXOOmkopc?si=F54C3YoWqGoL-mG-) (till 11:21 at 2x speed)  
## SVM:
Support Vector Machines (SVMs) is a powerful tool in data science for both classification tasks. You'll delve into the theory behind SVMs, understanding how they optimize decision boundaries to maximize margin and handle complex data distributions. Optional study includes polynomial kernels for non-linear data transformations. Through practical applications, you'll master SVMs for robust data analysis and prediction.

[Basic Intuition](https://youtu.be/H9yACitf-KM?si=JCrZzpSIFY0aaV0u) (at 1.5x speed)  
[Maths and Theory](https://youtu.be/Js3GLb1xPhc?si=AMAxwVn-cK6nKIL1) (at 1.5x speed)  
[Application](https://youtu.be/FB5EdxAGxQg?si=AiNDpo0t0f3zjQbP) (at 2x speed)  
[Polynomial Kernels SVM](https://youtu.be/8bFKyb77vp0?si=c7gLv7to7dy-NeCF) (Optional)  
[Polynomial Kernels Application](https://youtu.be/dl_ZsuHSIFE?si=vPj0FCbwNFBnqaY9) (Optional)
## Decision Trees & Random Forest:
Decision Trees and Random Forests are the fundamental and first-chosen techniques in data science for classification tasks. You'll learn how Decision Trees partition data based on features to make predictions, and how Random Forests aggregate multiple trees for improved accuracy and robustness. Through practical applications, you'll gain proficiency in building, evaluating, and optimizing these models for various data science tasks.

[Decision Tree in ML](https://youtu.be/RmajweUFKvM?si=kMkMDKvsUNn9GtJc&t=393) (6:32 to 14:31 at 1.5x speed)  
[Random Forest in ML](https://youtu.be/eM4uJ6XGnSM?si=U9eHNFiLz-TtABwv&t=858) (14:18 to 17:18 at 1.5x speed)  
[Random Forest Classifier and Regressor](https://youtu.be/nxFG5xdpDto?si=3ACkJrx7H-McFssa&t=272) (4:30 to end at 1.5x speed)  
[Application](https://youtu.be/ok2s1vV9XW0?si=Imgl-oYGEcBrIvGu&t=154) (2:43 to 10:36 at 2x speed)
## KNN:
K-nearest neighbors (KNN) is a non-parametric classification and regression method. It predicts new data points based on their similarity to existing data points, using a majority vote (for classification) or averaging (for regression) among its nearest neighbors in the feature space.

[KNN in ML](https://youtu.be/CQveSaMyEwM?si=-DrgOoalYD89MaSD) (Optional)
## Naive Bayes:
Naive Bayes is a probabilistic classifier based on Bayes' theorem. It assumes that features are independent of each other, hence "naive," which simplifies computation and training. It's efficient with minimal training data, making it ideal for tasks like text classification and spam filtering, where it often yields effective results despite its simplicity. Despite this simplification, Naive Bayes classifiers often perform well in practice, especially with high-dimensional data and large datasets.

[Intuition](https://youtu.be/jS1CKhALUBQ?si=8LC2Qn9oFexke3aM) (at 1.5x speed)  
[Naive Bayes on Text Data](https://youtu.be/temQ8mHpe3k?si=u_I8AyPMoRho6KHV) (at 1.5x speed)  
[Application](https://youtu.be/nHIUYwN-5rM?si=bHFlE9KX23WmaZH-) (at 2x speed)  
>[!Warning]
NLP is done with advanced Deep Learning algorithms
## K-means Clustering:
K-means clustering is a partitioning algorithm that aims to divide a dataset into K clusters by iteratively assigning each data point to the cluster with the nearest centroid and recalculating centroids until convergence. It's widely used for data segmentation and pattern recognition tasks.

[K-Means in ML](https://youtu.be/EItlUEPCIzM?si=f9Or7ZgnpzlJmudt) (Optional)

